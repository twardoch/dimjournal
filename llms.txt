This file is a merged representation of the entire codebase, combined into a single document by Repomix.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
5. Multiple file entries, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

</file_summary>

<directory_structure>
.github/
  workflows/
    ci.yml
src/
  dimjournal/
    __init__.py
    __main__.py
    dimjournal.py
tests/
  conftest.py
  test_dimjournal.py
.coveragerc
.gitignore
.isort.cfg
.pre-commit-config.yaml
AUTHORS.md
LICENSE.txt
pyproject.toml
README.md
setup.cfg
setup.py
tox.ini
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path=".github/workflows/ci.yml">
name: Test, Build, and Publish

on:
  push:
    branches: [main]
    tags: ['v[0-9]*', '[0-9]+.[0-9]+*']
  pull_request:
  workflow_dispatch:
  schedule:
    - cron: '0 0 1,16 * *'

permissions:
  contents: write

concurrency:
  group: >-
    ${{ github.workflow }}-${{ github.ref_type }}- ${{ github.event.pull_request.number || github.sha }}


  cancel-in-progress: true

jobs:
  test:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v2
      - name: Set up Python
        uses: actions/setup-python@v2
        with:
          python-version: '3.10'
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install pytest
          pip install -e .
      - name: Run tests
        run: pytest

  update-docs:
    needs: test
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v2
        with:
          ref: ${{ github.head_ref }}
      - name: Set up Python
        uses: actions/setup-python@v2
        with:
          python-version: '3.10'
      - name: Install pydoc-markdown
        run: pip install pydoc-markdown
      - name: Generate API documentation
        run: pydoc-markdown > API.md
      - name: Commit and push if it's changed
        uses: EndBug/add-and-commit@v9
        with:
          message: 'Update API documentation'
          add: 'API.md'

  build-and-publish:
    needs: test
    runs-on: ubuntu-latest
    if: startsWith(github.ref, 'refs/tags/')
    steps:
      - uses: actions/checkout@v2
      - name: Set up Python
        uses: actions/setup-python@v2
        with:
          python-version: '3.10'
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install build
      - name: Build distribution
        run: python -m build
      - name: Publish to PyPi
        uses: pypa/gh-action-pypi-publish@release/v1
        with:
          user: __token__
          password: ${{ secrets.PYPI_TOKEN }}
</file>

<file path="src/dimjournal/__init__.py">
import sys
from .dimjournal import download

if sys.version_info[:2] >= (3, 8):
    # TODO: Import directly (no need for conditional) when `python_requires = >= 3.8`
    from importlib.metadata import PackageNotFoundError, version  # pragma: no cover
else:
    from importlib_metadata import PackageNotFoundError, version  # pragma: no cover

try:
    # Change here if project is renamed and does not equal the package name
    dist_name = __name__
    __version__ = version(dist_name)
except PackageNotFoundError:  # pragma: no cover
    __version__ = "unknown"
finally:
    del version, PackageNotFoundError
</file>

<file path="src/dimjournal/__main__.py">
#!/usr/bin/env python3

import fire
from pathlib import Path
from .dimjournal import download


def cli():
    fire.core.Display = lambda lines, out: print(*lines, file=out)
    fire.Fire(download)


if __name__ == "__main__":
    cli()
</file>

<file path="src/dimjournal/dimjournal.py">
#!/usr/bin/env python3

import base64
import datetime as dt
import io
import itertools
import json
import logging
import pickle
from pathlib import Path
from typing import List
from urllib.parse import urlparse

import numpy as np
import pymtpng
import undetected_chromedriver as webdriver
from bs4 import BeautifulSoup
from PIL import Image
from selenium.common.exceptions import InvalidCookieDomainException
from selenium.webdriver.common.by import By
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.support.ui import WebDriverWait
from slugify import slugify
from tqdm import tqdm

_log = logging.getLogger("dimjournal")


class Constants:
    date_format = "%Y-%m-%d %H:%M:%S.%f"
    home_url = "https://www.midjourney.com/home/"
    app_url = "https://www.midjourney.com/app/"
    account_url = "https://www.midjourney.com/account/"
    api_url = "https://www.midjourney.com/api/app/recent-jobs/"
    session_token_cookie = "__Secure-next-auth.session-token"
    app_element_id = "app-root"
    account_element_id = "__NEXT_DATA__"
    job_details = ["id", "enqueue_time", "prompt"]
    user_json = Path("user.json")
    jobs_upscaled_json = Path("jobs_upscaled.json")
    cookies_pkl = Path("cookies.pkl")
    mj_download_image_js = """var callback=arguments[arguments.length-1];function getDataUri(e,n){var a=new XMLHttpRequest;a.onload=function(){var e=new FileReader;e.onloadend=function(){n(e.result)},e.readAsDataURL(a.response)},a.open("GET",e),a.responseType="blob",a.send()}getDataUri(document.querySelector("img").src,function(e){callback(e)});"""


def get_date_ninety_days_prior(date_string: str) -> str:
    """
    Get the date 90 days prior to the given date.

    Args:
        date_string (str): The date string in the format "%Y-%m-%d %H:%M:%S.%f".

    Returns:
        str: The date string 90 days prior to the given date.
    """
    DAYS_PRIOR = 90
    date_obj = dt.datetime.strptime(date_string, Constants.date_format)
    prev_day_obj = date_obj - dt.timedelta(days=DAYS_PRIOR)
    prev_day_string = prev_day_obj.strftime(Constants.date_format)
    return prev_day_string


class MidjourneyAPI:
    """
    A class to interact with the Midjourney API.

    Attributes:
        archive_folder (Path): The path to the archive folder.
        driver (webdriver.Chrome): The Chrome driver.
    """

    def __init__(self, driver: webdriver.Chrome, archive_folder: Path | str) -> None:
        """
        The constructor for the MidjourneyAPI class.

        Args:
            driver (webdriver.Chrome): The Chrome driver.
            archive_folder (Path | str): The path to the archive folder.
        """
        self.archive_folder = Path(archive_folder)
        self.driver = driver
        self.log_in()
        self.get_user_info()

    def log_in(self) -> bool:
        """
        Log in to the Midjourney API.

        Returns:
            bool: True if login is successful, False otherwise.
        """

    def load_cookies(self):
        self.cookies_path = Path(self.archive_folder, Constants.cookies_pkl)
        if self.cookies_path.is_file():
            cookies = pickle.load(open(self.cookies_path, "rb"))
            for cookie in cookies:
                try:
                    self.driver.add_cookie(cookie)
                except InvalidCookieDomainException:
                    pass

    def save_cookies(self):
        pickle.dump(self.driver.get_cookies(), open(self.cookies_path, "wb"))

    def log_in(self) -> bool:
        """
        Log in to the Midjourney API.

        Returns:
            bool: True if login is successful, False otherwise.
        """
        self.load_cookies()
        try:
            self.driver.get(Constants.home_url)
            WebDriverWait(self.driver, 60 * 10).until(EC.url_to_be(Constants.app_url))
            WebDriverWait(self.driver, 60 * 10).until(
                EC.presence_of_element_located((By.ID, Constants.app_element_id))
            )
            cookie = self.driver.get_cookie(Constants.session_token_cookie)
            if cookie is not None:
                self.save_cookies()
                self.session_token = cookie["value"]
                return True
            else:
                return False
        except Exception as e:
            _log.error(f"Failed to get session token: {str(e)}")
            return False

    def load_user_info(self):
        self.user_info = {}
        self.user_json = Path(self.archive_folder, Constants.user_json)
        if self.user_json.is_file():
            self.user_info = json.loads(self.user_json.read_text())
        else:
            self.user_info = self.fetch_user_info()
            if self.user_info:
                self.user_json.write_text(json.dumps(self.user_info))

    def fetch_user_info(self):
        try:
            self.driver.get(Constants.account_url)
            WebDriverWait(self.driver, 60 * 10).until(
                EC.presence_of_element_located((By.ID, Constants.account_element_id))
            )
            soup = BeautifulSoup(self.driver.page_source, "html.parser")
            script_tag_contents = soup.find(
                "script", id=Constants.account_element_id
            ).text
            return json.loads(script_tag_contents)
        except Exception as e:
            _log.error(f"Failed to get user info: {str(e)}")
            return None

    def get_user_info(self) -> bool:
        self.load_user_info()
        if self.user_info:
            self.user_id = self.user_info["props"]["pageProps"]["user"]["id"]
            return True
        else:
            return False

    def request_recent_jobs(
        self,
        from_date: str | None = None,
        page: int | None = None,
        job_type: str | None = None,
        amount: int = 50,
    ) -> List[dict]:
        """
        Request recent jobs from the Midjourney API.

        Args:
            from_date (str | None): The date from which to request jobs.
            page (int | None): The page number to request.
            job_type (str | None): The type of job to request.
            amount (int): The number of jobs to request.

        Returns:
            List[dict]: A list of jobs.
        """
        params = {}
        if from_date:
            pass  # params["fromDate"] = prev_day(from_date)
        if page:
            params["page"] = page
        if job_type:
            params["jobType"] = job_type
        params["amount"] = amount
        params["orderBy"] = "new"
        params["jobStatus"] = "completed"
        params["userId"] = self.user_id
        params["dedupe"] = "true"
        params["refreshApi"] = 0

        query_string = "&".join([f"{k}={v}" for k, v in params.items()])
        url = f"{Constants.api_url}?{query_string}"

        _log.debug(f"Requesting {url}")
        self.driver.get(url)
        soup = BeautifulSoup(self.driver.page_source, "html.parser")
        pre_tag_contents = soup.find("pre").text
        job_listing = json.loads(pre_tag_contents)

        if isinstance(job_listing, list):
            if len(job_listing) > 0 and isinstance(job_listing[0], dict):
                if all(f in job_listing[0] for f in Constants.job_details):
                    _log.debug(f"Got job listing with {len(job_listing)} jobs")
                    return job_listing
                if job_listing[0] == {"msg": "No jobs found."}:
                    _log.debug(f"Response: 'No jobs found'")
                    return []
            elif len(job_listing) == 0:
                _log.debug(f"Response: 'No jobs found'")
                return []
        raise ValueError(job_listing)


class MidjourneyJobCrawler:
    def __init__(
        self, api: MidjourneyAPI, archive_folder: Path, job_type: str | None = None
    ):
        """
        The constructor for the MidjourneyJobCrawler class.

        Args:
            api (MidjourneyAPI): The Midjourney API.
            archive_folder (Path): The path to the archive folder.
            job_type (Optional[str]): The type of job to crawl.
        """
        self.api = api
        self.job_type = job_type
        self.archive_folder = archive_folder
        if job_type:
            self.archive_file = Path(f"jobs_{job_type}.json")
        else:
            self.archive_file = Path("jobs.json")
        self.archive_file = self.archive_folder / self.archive_file
        self.archive_data = []

    def load_archive_data(self):
        """
        Load the archive data.
        """
        if self.archive_file.is_file():
            self.archive_data = json.loads(self.archive_file.read_text())
        else:
            self.archive_data = []

    def update_archive_data(self, job_listing: List[dict]):
        """
        Update the archive data with the given job listing.

        Args:
            job_listing (List[dict]): The job listing.

        Returns:
            bool: True if the archive data was updated, False otherwise.
        """
        new_entries = [
            job
            for job in job_listing
            if job["id"] not in [x["id"] for x in self.archive_data]
        ]
        if new_entries:
            self.archive_data.extend(new_entries)
            self.archive_file.write_text(json.dumps(self.archive_data, indent=2))
        else:
            return False
        return True

    def crawl(
        self,
        limit: int | None = None,
        from_date: str | None = None,
    ):
        """
        Crawl the Midjourney API for job listings.

        Args:
            limit (Optional[int]): The maximum number of pages to crawl.
            from_date (Optional[str]): The date from which to start crawling.
        """
        job_str = self.job_type if self.job_type else "all"
        self.load_archive_data()
        pages = range(1, limit + 1) if limit else itertools.count(1)
        for page in tqdm(pages, desc=f"Crawling for {job_str} job info"):
            job_listing = self.api.request_recent_jobs(
                from_date=from_date, page=page, job_type=self.job_type
            )
            if not job_listing:
                _log.debug(
                    f"Empty {job_str} job listing batch: reached end of total job listing"
                )
                break
            if not self.update_archive_data(job_listing):
                _log.debug(f"No new {job_str} jobs found: stopping crawler")
                break
            if from_date is None:
                from_date = job_listing[0]["enqueue_time"]


class MidjourneyDownloader:
    def __init__(self, api, archive_folder):
        """
        The constructor for the MidjourneyDownloader class.

        Args:
            api (MidjourneyAPI): The Midjourney API.
            archive_folder (Path): The path to the archive folder.
        """
        self.api = api
        self.archive_folder = Path(archive_folder)
        self.archive_folder.mkdir(parents=True, exist_ok=True)
        self.jobs_json_path = Path(self.archive_folder, "jobs_upscale.json")
        self.jobs_upscale = self.read_jobs()

    def fetch_image(self, url):
        """
        Fetch an image from the given URL.

        Args:
            url (str): The URL of the image.

        Returns:
            Tuple[bytes, str]: The image data and the image type.
        """
        self.api.driver.get(url)
        data_uri = self.api.driver.execute_async_script(Constants.mj_download_image_js)
        header, encoded = data_uri.split(",", 1)
        image_type = header.split(";")[0].split("/")[-1]
        image_data = base64.b64decode(encoded)
        return image_data, image_type

    def read_jobs(self):
        """
        Read the job listings.

        Returns:
            List[dict]: The job listings.
        """
        with open(self.jobs_json_path, "r") as file:
            return json.load(file)

    def save_jobs(self):
        """
        Save the job listings.
        """
        with open(self.jobs_json_path, "w") as file:
            json.dump(self.jobs_upscale, file, indent=2)
        _log.debug(f"Updated {self.jobs_json_path}")

    def create_folders(self, dt_obj):
        """
        Create folders for the given date.

        Args:
            dt_obj (datetime): The date.

        Returns:
            Path: The path to the created folder.
        """
        dt_year = f"{dt_obj.year}"
        path_year = Path(self.archive_folder, dt_year)
        path_year.mkdir(parents=True, exist_ok=True)
        month = f"{dt_obj.month:02}"
        path_month = Path(path_year, month)
        path_month.mkdir(parents=True, exist_ok=True)
        return path_month

    def fetch_and_write_image(self, image_url, image_path, info):
        """
        Fetch an image from the given URL and write it to the given path.

        Args:
            image_url (str): The URL of the image.
            image_path (Path): The path to write the image.
            info (dict): The metadata of the image.

        Returns:
            bool: True if the image was successfully fetched and written, False otherwise.
        """
        if not image_path.is_file():
            image_data, image_type = self.fetch_image(image_url)
            if image_type == "png":
                try:
                    image_array = np.array(Image.open(io.BytesIO(image_data)))
                    with open(image_path, "wb") as fh:
                        pymtpng.encode_png(image_array, fh, info=info)
                except Exception as e:
                    _log.error(f"Fishy PNG: {image_url}")
                    with open(image_path, "wb") as fh:
                        fh.write(image_data)
            else:
                with open(image_path, "wb") as fh:
                    fh.write(image_data)
            return True
        else:
            return False

    def download_missing(self):
        """
        Download missing images.
        """

        with tqdm(total=len(self.jobs_upscale), desc="Downloading") as pbar:
            last_tick = 0
            for job_i, job in enumerate(self.jobs_upscale):
                if not job.get("arch", False):
                    dt_obj = dt.datetime.strptime(
                        job["enqueue_time"], Constants.date_format
                    )
                    path_month = self.create_folders(dt_obj)

                    dt_stamp = dt_obj.strftime("%Y%m%d-%H%M")
                    prompt = job.get("prompt", "") or job.get("full_command", "") or ""
                    image_url = job["image_paths"][0]

                    job["arch_prompt_slug"] = slugify(prompt)[:49]
                    path_base = (
                        f"""{dt_stamp}_{job["arch_prompt_slug"]}_{job["id"][:4]}"""
                    )
                    path_ext = Path(urlparse(image_url).path).suffix[1:]
                    image_path = Path(path_month, f"""{path_base}.{path_ext}""")
                    info = {
                        "Title": job.get("prompt", ""),
                        "Author": job.get("username", ""),
                        "Description": job.get("full_command", ""),
                        "Copyright": job.get("username", ""),
                        "Creation Time": job.get("enqueue_time", ""),
                        "Software": "Midjourney",
                    }
                    if self.fetch_and_write_image(image_url, image_path, info):
                        job["arch"] = True
                        job["arch_image_path"] = str(
                            image_path.relative_to(self.archive_folder)
                        )

                        pbar.set_description(f"{job['arch_image_path']}")
                        pbar.update(job_i + 1 - last_tick)
                        last_tick = job_i + 1
                        _log.debug(
                            f"""Saving {job["arch_image_path"]} from {image_url}"""
                        )
        self.save_jobs()


def download(
    archive_folder: Path | str | None = None,
    user_id: str | None = None,
    limit: int | None = None,
):
    """
    Download images from the Midjourney API.

    Args:
        archive_folder (Optional[Union[Path, str]]): The path to the archive folder.
        user_id (Optional[str]): The user ID.
        limit (Optional[int]): The maximum number of pages to download.
    """
    logging.basicConfig(level=logging.INFO)
    import os

    pictures_folder = "My Pictures" if os.name == "nt" else "Pictures"
    archive_folder = (
        Path(archive_folder)
        if archive_folder
        else Path(os.path.expanduser("~"), pictures_folder, "midjourney", "dimjournal")
    )
    if not archive_folder.is_dir():
        archive_folder.mkdir(parents=True)
    _log.info(f"Data will be saved in: {archive_folder}")
    options = webdriver.ChromeOptions()
    driver = webdriver.Chrome(use_subprocess=True, options=options)
    api = MidjourneyAPI(driver=driver, archive_folder=archive_folder)

    try:
        crawler = MidjourneyJobCrawler(api, archive_folder, job_type="upscale")
        crawler.crawl(limit=limit)
        crawler = MidjourneyJobCrawler(api, archive_folder, job_type=None)
        crawler.crawl(limit=limit)
        downloader = MidjourneyDownloader(api, archive_folder)
        downloader.download_missing()
    except KeyboardInterrupt:
        _log.warn("Caught KeyboardInterrupt")
    finally:
        driver.quit()
</file>

<file path="tests/conftest.py">
"""
    Dummy conftest.py for dimjournal.

    If you don't know what this is for, just leave it empty.
    Read more about conftest.py under:
    - https://docs.pytest.org/en/stable/fixture.html
    - https://docs.pytest.org/en/stable/writing_plugins.html
"""

# import pytest
</file>

<file path="tests/test_dimjournal.py">
import pytest
from dimjournal.dimjournal import (
    MidjourneyAPI,
    MidjourneyJobCrawler,
    MidjourneyDownloader,
)
from pathlib import Path
import undetected_chromedriver as webdriver


def test():
    assert True
</file>

<file path=".coveragerc">
# .coveragerc to control coverage.py
[run]
branch = True
source = dimjournal
# omit = bad_file.py

[paths]
source =
    src/
    */site-packages/

[report]
# Regexes for lines to exclude from consideration
exclude_lines =
    # Have to re-enable the standard pragma
    pragma: no cover

    # Don't complain about missing debug-only code:
    def __repr__
    if self\.debug

    # Don't complain if tests don't hit defensive assertion code:
    raise AssertionError
    raise NotImplementedError

    # Don't complain if non-runnable code isn't run:
    if 0:
    if __name__ == .__main__.:
</file>

<file path=".gitignore">
.aider*
# Temporary and binary files
*~
*.py[cod]
*.so
*.cfg
!.isort.cfg
!setup.cfg
*.orig
*.log
*.pot
__pycache__/*
.cache/*
.*.swp
*/.ipynb_checkpoints/*
.DS_Store

# Project files
.ropeproject
.project
.pydevproject
.settings
.idea
.vscode
tags

# Package files
*.egg
*.eggs/
.installed.cfg
*.egg-info

# Unittest and coverage
htmlcov/*
.coverage
.coverage.*
.tox
junit*.xml
coverage.xml
.pytest_cache/

# Build and docs folder/files
build/*
dist/*
sdist/*
docs/api/*
docs/_rst/*
docs/_build/*
cover/*
MANIFEST

# Per-project virtualenvs
.venv*/
.conda*/
.python-version
</file>

<file path=".isort.cfg">
[settings]
profile = black
known_first_party = dimjournal
</file>

<file path=".pre-commit-config.yaml">
exclude: '^docs/conf.py'

repos:
- repo: https://github.com/pre-commit/pre-commit-hooks
  rev: v4.4.0
  hooks:
  - id: trailing-whitespace
  - id: check-added-large-files
  - id: check-ast
  - id: check-json
  - id: check-merge-conflict
  - id: check-xml
  - id: check-yaml
  - id: debug-statements
  - id: end-of-file-fixer
  - id: requirements-txt-fixer
  - id: mixed-line-ending
    args: ['--fix=auto']  # replace 'auto' with 'lf' to enforce Linux/Mac line endings or 'crlf' for Windows

## If you want to automatically "modernize" your Python code:
# - repo: https://github.com/asottile/pyupgrade
#   rev: v3.3.1
#   hooks:
#   - id: pyupgrade
#     args: ['--py37-plus']

## If you want to avoid flake8 errors due to unused vars or imports:
# - repo: https://github.com/PyCQA/autoflake
#   rev: v2.0.2
#   hooks:
#   - id: autoflake
#     args: [
#       --in-place,
#       --remove-all-unused-imports,
#       --remove-unused-variables,
#     ]

- repo: https://github.com/PyCQA/isort
  rev: 5.12.0
  hooks:
  - id: isort

- repo: https://github.com/psf/black
  rev: 23.3.0
  hooks:
  - id: black
    language_version: python3

## If like to embrace black styles even in the docs:
# - repo: https://github.com/asottile/blacken-docs
#   rev: v1.13.0
#   hooks:
#   - id: blacken-docs
#     additional_dependencies: [black]

- repo: https://github.com/PyCQA/flake8
  rev: 6.0.0
  hooks:
  - id: flake8
  ## You can add flake8 plugins via `additional_dependencies`:
  #  additional_dependencies: [flake8-bugbear]

## Check for misspells in documentation files:
# - repo: https://github.com/codespell-project/codespell
#   rev: v2.2.4
#   hooks:
#   - id: codespell
</file>

<file path="AUTHORS.md">
# Contributors

* Adam Twardoch <adam+github@twardoch.com>
</file>

<file path="LICENSE.txt">
Apache License
                           Version 2.0, January 2004
                        http://www.apache.org/licenses/

   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION

   1. Definitions.

      "License" shall mean the terms and conditions for use, reproduction,
      and distribution as defined by Sections 1 through 9 of this document.

      "Licensor" shall mean the copyright owner or entity authorized by
      the copyright owner that is granting the License.

      "Legal Entity" shall mean the union of the acting entity and all
      other entities that control, are controlled by, or are under common
      control with that entity. For the purposes of this definition,
      "control" means (i) the power, direct or indirect, to cause the
      direction or management of such entity, whether by contract or
      otherwise, or (ii) ownership of fifty percent (50%) or more of the
      outstanding shares, or (iii) beneficial ownership of such entity.

      "You" (or "Your") shall mean an individual or Legal Entity
      exercising permissions granted by this License.

      "Source" form shall mean the preferred form for making modifications,
      including but not limited to software source code, documentation
      source, and configuration files.

      "Object" form shall mean any form resulting from mechanical
      transformation or translation of a Source form, including but
      not limited to compiled object code, generated documentation,
      and conversions to other media types.

      "Work" shall mean the work of authorship, whether in Source or
      Object form, made available under the License, as indicated by a
      copyright notice that is included in or attached to the work
      (an example is provided in the Appendix below).

      "Derivative Works" shall mean any work, whether in Source or Object
      form, that is based on (or derived from) the Work and for which the
      editorial revisions, annotations, elaborations, or other modifications
      represent, as a whole, an original work of authorship. For the purposes
      of this License, Derivative Works shall not include works that remain
      separable from, or merely link (or bind by name) to the interfaces of,
      the Work and Derivative Works thereof.

      "Contribution" shall mean any work of authorship, including
      the original version of the Work and any modifications or additions
      to that Work or Derivative Works thereof, that is intentionally
      submitted to Licensor for inclusion in the Work by the copyright owner
      or by an individual or Legal Entity authorized to submit on behalf of
      the copyright owner. For the purposes of this definition, "submitted"
      means any form of electronic, verbal, or written communication sent
      to the Licensor or its representatives, including but not limited to
      communication on electronic mailing lists, source code control systems,
      and issue tracking systems that are managed by, or on behalf of, the
      Licensor for the purpose of discussing and improving the Work, but
      excluding communication that is conspicuously marked or otherwise
      designated in writing by the copyright owner as "Not a Contribution."

      "Contributor" shall mean Licensor and any individual or Legal Entity
      on behalf of whom a Contribution has been received by Licensor and
      subsequently incorporated within the Work.

   2. Grant of Copyright License. Subject to the terms and conditions of
      this License, each Contributor hereby grants to You a perpetual,
      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
      copyright license to reproduce, prepare Derivative Works of,
      publicly display, publicly perform, sublicense, and distribute the
      Work and such Derivative Works in Source or Object form.

   3. Grant of Patent License. Subject to the terms and conditions of
      this License, each Contributor hereby grants to You a perpetual,
      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
      (except as stated in this section) patent license to make, have made,
      use, offer to sell, sell, import, and otherwise transfer the Work,
      where such license applies only to those patent claims licensable
      by such Contributor that are necessarily infringed by their
      Contribution(s) alone or by combination of their Contribution(s)
      with the Work to which such Contribution(s) was submitted. If You
      institute patent litigation against any entity (including a
      cross-claim or counterclaim in a lawsuit) alleging that the Work
      or a Contribution incorporated within the Work constitutes direct
      or contributory patent infringement, then any patent licenses
      granted to You under this License for that Work shall terminate
      as of the date such litigation is filed.

   4. Redistribution. You may reproduce and distribute copies of the
      Work or Derivative Works thereof in any medium, with or without
      modifications, and in Source or Object form, provided that You
      meet the following conditions:

      (a) You must give any other recipients of the Work or
          Derivative Works a copy of this License; and

      (b) You must cause any modified files to carry prominent notices
          stating that You changed the files; and

      (c) You must retain, in the Source form of any Derivative Works
          that You distribute, all copyright, patent, trademark, and
          attribution notices from the Source form of the Work,
          excluding those notices that do not pertain to any part of
          the Derivative Works; and

      (d) If the Work includes a "NOTICE" text file as part of its
          distribution, then any Derivative Works that You distribute must
          include a readable copy of the attribution notices contained
          within such NOTICE file, excluding those notices that do not
          pertain to any part of the Derivative Works, in at least one
          of the following places: within a NOTICE text file distributed
          as part of the Derivative Works; within the Source form or
          documentation, if provided along with the Derivative Works; or,
          within a display generated by the Derivative Works, if and
          wherever such third-party notices normally appear. The contents
          of the NOTICE file are for informational purposes only and
          do not modify the License. You may add Your own attribution
          notices within Derivative Works that You distribute, alongside
          or as an addendum to the NOTICE text from the Work, provided
          that such additional attribution notices cannot be construed
          as modifying the License.

      You may add Your own copyright statement to Your modifications and
      may provide additional or different license terms and conditions
      for use, reproduction, or distribution of Your modifications, or
      for any such Derivative Works as a whole, provided Your use,
      reproduction, and distribution of the Work otherwise complies with
      the conditions stated in this License.

   5. Submission of Contributions. Unless You explicitly state otherwise,
      any Contribution intentionally submitted for inclusion in the Work
      by You to the Licensor shall be under the terms and conditions of
      this License, without any additional terms or conditions.
      Notwithstanding the above, nothing herein shall supersede or modify
      the terms of any separate license agreement you may have executed
      with Licensor regarding such Contributions.

   6. Trademarks. This License does not grant permission to use the trade
      names, trademarks, service marks, or product names of the Licensor,
      except as required for reasonable and customary use in describing the
      origin of the Work and reproducing the content of the NOTICE file.

   7. Disclaimer of Warranty. Unless required by applicable law or
      agreed to in writing, Licensor provides the Work (and each
      Contributor provides its Contributions) on an "AS IS" BASIS,
      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
      implied, including, without limitation, any warranties or conditions
      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A
      PARTICULAR PURPOSE. You are solely responsible for determining the
      appropriateness of using or redistributing the Work and assume any
      risks associated with Your exercise of permissions under this License.

   8. Limitation of Liability. In no event and under no legal theory,
      whether in tort (including negligence), contract, or otherwise,
      unless required by applicable law (such as deliberate and grossly
      negligent acts) or agreed to in writing, shall any Contributor be
      liable to You for damages, including any direct, indirect, special,
      incidental, or consequential damages of any character arising as a
      result of this License or out of the use or inability to use the
      Work (including but not limited to damages for loss of goodwill,
      work stoppage, computer failure or malfunction, or any and all
      other commercial damages or losses), even if such Contributor
      has been advised of the possibility of such damages.

   9. Accepting Warranty or Additional Liability. While redistributing
      the Work or Derivative Works thereof, You may choose to offer,
      and charge a fee for, acceptance of support, warranty, indemnity,
      or other liability obligations and/or rights consistent with this
      License. However, in accepting such obligations, You may act only
      on Your own behalf and on Your sole responsibility, not on behalf
      of any other Contributor, and only if You agree to indemnify,
      defend, and hold each Contributor harmless for any liability
      incurred by, or claims asserted against, such Contributor by reason
      of your accepting any such warranty or additional liability.

   END OF TERMS AND CONDITIONS

   APPENDIX: How to apply the Apache License to your work.

      To apply the Apache License to your work, attach the following
      boilerplate notice, with the fields enclosed by brackets "{}"
      replaced with your own identifying information. (Don't include
      the brackets!)  The text should be enclosed in the appropriate
      comment syntax for the file format. We also recommend that a
      file or class name and description of purpose be included on the
      same "printed page" as the copyright notice for easier
      identification within third-party archives.

   Copyright {yyyy} {name of copyright owner}

   Licensed under the Apache License, Version 2.0 (the "License");
   you may not use this file except in compliance with the License.
   You may obtain a copy of the License at

       http://www.apache.org/licenses/LICENSE-2.0

   Unless required by applicable law or agreed to in writing, software
   distributed under the License is distributed on an "AS IS" BASIS,
   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   See the License for the specific language governing permissions and
   limitations under the License.
</file>

<file path="pyproject.toml">
[build-system]
# AVOID CHANGING REQUIRES: IT WILL BE UPDATED BY PYSCAFFOLD!
requires = ["setuptools>=46.1.0", "setuptools_scm[toml]>=5"]
build-backend = "setuptools.build_meta"

[tool.setuptools_scm]
# For smarter version schemes and other configuration options,
# check out https://github.com/pypa/setuptools_scm
version_scheme = "no-guess-dev"
</file>

<file path="README.md">
# Dimjournal

Dimjournal is a Midjourney backup tool. It automatically downloads the metadata archive and the upscaled images from Midjourney into a local archive (folder tree). It also embeds some basic metadata (like the prompt) into the PNG files.

_Note: the terms of use of Midjourney disallow any automation._

Dimjournal is a Python tool uses the Selenium WebDriver to log into the Midjourney website, fetch user data, and download job information and images.

## Changelog

- v1.0.8: Fixes
- v1.0.3: Tested on macOS in July 2023

## Installation

Stable version:

```
pip install dimjournal
```

Development version:

```
python3 -m pip install git+https://github.com/twardoch/dimjournal
```

## Usage

### Command Line Interface (CLI)

In Terminal, run:

```bash
dimjournal
```

Dimjournal will open a browser where you need to log into MidJourney. The tool will create the backup folder, which by default is the `midjourney/dimjournal` subfolder inside your `Pictures`/`My Pictures` folder. It will operate the browser, download all metadata (up to 2,500 last upscale jobs, and up to 2,500 jobs), and save it in JSON files in the backup folder. Then it will use the browser to download all upscales that are not in the backup folder. If you run the tool again, it will only download new metadata, and new images.

To specify a different backup folder, use:

```bash
python3 -m dimjournal --archive_folder /path/to/your/archive/folder
```

### Python

You can also use Dimjournal in your Python scripts. Here is an example of how to import and use the `download` function:

```python
from dimjournal import download

# Specify the directory where you want to store the data
archive_folder = "/path/to/your/archive/folder"

# Download the data
download(archive_folder)
```

## License

- Licensed under the [Apache-2.0 License](./LICENSE.txt)
- Written with assistance from ChatGPT
</file>

<file path="setup.cfg">
[metadata]
name = dimjournal
description = Archive utility for Midjourney
author = Adam Twardoch
author_email = adam+github@twardoch.com
license = Apache-2.0
license_files = LICENSE.txt
long_description = file: README.md
long_description_content_type = text/markdown; charset=UTF-8; variant=GFM
url = https://pypi.org/project/dimjournal/
project_urls =
    Documentation = https://twardoch.github.io/dimjournal/
    Source = https://github.com/twardoch/dimjournal
    Download = https://pypi.org/project/dimjournal
platforms = any
classifiers =
    Development Status :: 4 - Beta
    Programming Language :: Python
    Topic :: Software Development :: Libraries :: Python Modules
    License :: OSI Approved :: Apache Software License
    Operating System :: OS Independent
python_requires = >=3.10

[options]
zip_safe = False
packages = find_namespace:
include_package_data = True
package_dir =
    =src
install_requires =
    beautifulsoup4>=4.12.2
    fire>=0.5.0
    numpy>=1.25.0
    Pillow>=10.0.0
    pymtpng>=1.0
    pytest>=7.3.1
    python-slugify>=8.0.1
    selenium>=4.10.0
    setuptools>=67.6.1
    tqdm>=4.65.0
    undetected_chromedriver>=3.5.0

[options.packages.find]
where = src
exclude =
    tests

[options.extras_require]
testing =
    setuptools
    pytest
    pytest-cov

[options.entry_points]
console_scripts =
    dimjournal = dimjournal.__main__:cli

[tool:pytest]
addopts =
    --verbose
norecursedirs =
    dist
    build
    .tox
testpaths = tests

[devpi:upload]
no_vcs = 1
formats = bdist_wheel

[flake8]
max_line_length = 88
extend_ignore = E203, W503
exclude =
    .tox
    build
    dist
    .eggs
    docs/conf.py

[pyscaffold]
version = 4.4.1
package = dimjournal
extensions =
    github_actions
    markdown
    pre_commit
</file>

<file path="setup.py">
"""
    Setup file for dimjournal.
    Use setup.cfg to configure your project.

    This file was generated with PyScaffold 4.4.1.
    PyScaffold helps you to put up the scaffold of your new Python project.
    Learn more under: https://pyscaffold.org/
"""
from setuptools import setup

if __name__ == "__main__":
    try:
        setup(use_scm_version={"version_scheme": "no-guess-dev"})
    except:  # noqa
        print(
            "\n\nAn error occurred while building the project, "
            "please ensure you have the most updated version of setuptools, "
            "setuptools_scm and wheel with:\n"
            "   pip install -U setuptools setuptools_scm wheel\n\n"
        )
        raise
</file>

<file path="tox.ini">
# Tox configuration file
# Read more under https://tox.wiki/
# THIS SCRIPT IS SUPPOSED TO BE AN EXAMPLE. MODIFY IT ACCORDING TO YOUR NEEDS!

[tox]
minversion = 3.24
envlist = default
isolated_build = True


[testenv]
description = Invoke pytest to run automated tests
setenv =
    TOXINIDIR = {toxinidir}
passenv =
    HOME
    SETUPTOOLS_*
extras =
    testing
commands =
    pytest {posargs}


# # To run `tox -e lint` you need to make sure you have a
# # `.pre-commit-config.yaml` file. See https://pre-commit.com
# [testenv:lint]
# description = Perform static analysis and style checks
# skip_install = True
# deps = pre-commit
# passenv =
#     HOMEPATH
#     PROGRAMDATA
#     SETUPTOOLS_*
# commands =
#     pre-commit run --all-files {posargs:--show-diff-on-failure}


[testenv:{build,clean}]
description =
    build: Build the package in isolation according to PEP517, see https://github.com/pypa/build
    clean: Remove old distribution files and temporary build artifacts (./build and ./dist)
# https://setuptools.pypa.io/en/stable/build_meta.html#how-to-use-it
skip_install = True
changedir = {toxinidir}
deps =
    build: build[virtualenv]
passenv =
    SETUPTOOLS_*
commands =
    clean: python -c 'import shutil; [shutil.rmtree(p, True) for p in ("build", "dist", "docs/_build")]'
    clean: python -c 'import pathlib, shutil; [shutil.rmtree(p, True) for p in pathlib.Path("src").glob("*.egg-info")]'
    build: python -m build {posargs}
# By default, both `sdist` and `wheel` are built. If your sdist is too big or you don't want
# to make it available, consider running: `tox -e build -- --wheel`


[testenv:{docs,doctests,linkcheck}]
description =
    docs: Invoke sphinx-build to build the docs
    doctests: Invoke sphinx-build to run doctests
    linkcheck: Check for broken links in the documentation
passenv =
    SETUPTOOLS_*
setenv =
    DOCSDIR = {toxinidir}/docs
    BUILDDIR = {toxinidir}/docs/_build
    docs: BUILD = html
    doctests: BUILD = doctest
    linkcheck: BUILD = linkcheck
deps =
    -r {toxinidir}/docs/requirements.txt
    # ^  requirements.txt shared with Read The Docs
commands =
    sphinx-build --color -b {env:BUILD} -d "{env:BUILDDIR}/doctrees" "{env:DOCSDIR}" "{env:BUILDDIR}/{env:BUILD}" {posargs}


[testenv:publish]
description =
    Publish the package you have been developing to a package index server.
    By default, it uses testpypi. If you really want to publish your package
    to be publicly accessible in PyPI, use the `-- --repository pypi` option.
skip_install = True
changedir = {toxinidir}
passenv =
    # See: https://twine.readthedocs.io/en/latest/
    TWINE_USERNAME
    TWINE_PASSWORD
    TWINE_REPOSITORY
    TWINE_REPOSITORY_URL
deps = twine
commands =
    python -m twine check dist/*
    python -m twine upload {posargs:--repository {env:TWINE_REPOSITORY:testpypi}} dist/*
</file>

</files>
